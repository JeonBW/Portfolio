{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [1]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.cast(hypot>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.38513637]\n",
      " [0.9006001 ]\n",
      " [0.8868617 ]\n",
      " [0.9912576 ]\n",
      " [0.9253774 ]\n",
      " [0.9935975 ]\n",
      " [0.99445593]\n",
      " [0.99955463]]\n",
      "예측 :[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.00613043]\n",
      " [0.03419536]\n",
      " [0.03860086]\n",
      " [0.18730241]\n",
      " [0.03116208]\n",
      " [0.1731222 ]\n",
      " [0.15585297]\n",
      " [0.5458263 ]]\n",
      "예측 :[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [0], [0], [0], [0], [0], [0], [1]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.7423169 ]\n",
      " [0.7439075 ]\n",
      " [0.7484958 ]\n",
      " [0.75006115]\n",
      " [0.7479414 ]\n",
      " [0.7540301 ]\n",
      " [0.749509  ]\n",
      " [0.7555722 ]]\n",
      "예측 :[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,1]), tf.float32, name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias\")\n",
    "\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot) + (1 - y) * tf.log(1 - hypot))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))\n",
    "    \n",
    "# 제대로 예측이 되지 않음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM (XOR Gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics \n",
    "\n",
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = [0, 1, 1, 1, 1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C = 100)\n",
    "clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "examples = [[0, 0, 0], [1, 1, 1], [0, 1, 0 ], [1, 1, 0]]\n",
    "examples_label=[0, 0, 1, 1]\n",
    "\n",
    "result = clf.predict(examples)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(examples_label, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 딥러닝을 이용한 XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.04406121]\n",
      " [0.99155045]\n",
      " [0.9964491 ]\n",
      " [0.9675958 ]\n",
      " [0.9724196 ]\n",
      " [0.9633676 ]\n",
      " [0.97360265]\n",
      " [0.09750521]]\n",
      "예측 :[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 백 프로게이션 (레이어를 여러개 만들어서)\n",
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "# 첫번째 레이어 \n",
    "W1 = tf.Variable(tf.random_normal([3,10]), tf.float32, name=\"weight1\")\n",
    "# 바로 출력계층으로 넘겨주는게 아니라 2번째 레이어에 출력하는 것이므로 출력갯수를 조정할 수 있다.\n",
    "b1 = tf.Variable(tf.random_normal([10]),tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1) # X 실제 입력값\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([10,1]), tf.float32, name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias2\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2) # hypot1 : 첫번째 레이어로부터 입력받은 값\n",
    "# 2개의 레이어 연결 \n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a = sess.run([train, hypot2, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.00481015]\n",
      " [0.99699974]\n",
      " [0.99767387]\n",
      " [0.9963697 ]\n",
      " [0.9974313 ]\n",
      " [0.99769807]\n",
      " [0.9976884 ]\n",
      " [0.00876293]]\n",
      "예측 :[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 레이어(wide)의 수는 7개의 입출력 연결(Depp)의 수는 50개로 구현\n",
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "# 첫번째 레이어 \n",
    "W1 = tf.Variable(tf.random_normal([3,50]), tf.float32, name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias1\")\n",
    "hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1) # X 실제 입력값\n",
    "\n",
    "# 두번째 레이어 \n",
    "W2 = tf.Variable(tf.random_normal([50,50]), tf.float32, name=\"weight1\")\n",
    "b2 = tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias1\")\n",
    "hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2) \n",
    "\n",
    "# 세번째 레이어 \n",
    "W3 = tf.Variable(tf.random_normal([50,50]), tf.float32, name=\"weight2\")\n",
    "b3 = tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias2\")\n",
    "hypot3 = tf.sigmoid(tf.matmul(hypot2, W3) + b3) \n",
    "\n",
    "# 네번째 레이어 \n",
    "W4 = tf.Variable(tf.random_normal([50,50]), tf.float32, name=\"weight2\")\n",
    "b4= tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias2\")\n",
    "hypot4 = tf.sigmoid(tf.matmul(hypot3, W4) + b4)\n",
    "\n",
    "# 다섯번째 레이어 \n",
    "W5 = tf.Variable(tf.random_normal([50,50]), tf.float32, name=\"weight2\")\n",
    "b5 = tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias2\")\n",
    "hypot5 = tf.sigmoid(tf.matmul(hypot4, W5) + b5)\n",
    "\n",
    "# 여섯번째 레이어 \n",
    "W6 = tf.Variable(tf.random_normal([50,50]), tf.float32, name=\"weight2\")\n",
    "b6 = tf.Variable(tf.random_normal([50]),tf.float32, name=\"bias2\")\n",
    "hypot6 = tf.sigmoid(tf.matmul(hypot5, W6) + b6)\n",
    "\n",
    "# 일곱번째 레이어 (출력)\n",
    "W7 = tf.Variable(tf.random_normal([50,1]), tf.float32, name=\"weight2\")\n",
    "b7 = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias2\")\n",
    "hypot7 = tf.sigmoid(tf.matmul(hypot6, W7) + b7)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypot7) + (1 - y) * tf.log(1 - hypot7))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot7>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        _, h, p, a = sess.run([train, hypot7, pred, accuracy], feed_dict={X:x_data, y:y_data})\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))\n",
    "# 훨씬더 안전하고 정확하게 학습할 수 있다 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "- 어떤 정보를 보고싶은지 결정\n",
    "- 선택한 정보들을 하나로 추합\n",
    "- 새로운 그래프를 그릴수 있게 추가\n",
    "- 파일로 저장\n",
    "- 웹브라우저로 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.04084513]\n",
      " [0.97623724]\n",
      " [0.9845428 ]\n",
      " [0.9771501 ]\n",
      " [0.99127936]\n",
      " [0.97235507]\n",
      " [0.9732654 ]\n",
      " [0.08556792]]\n",
      "예측 :[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "정확도 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#새로갱신\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "# 첫번째 레이어 \n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3,10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]),tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1) # X 실제 입력값\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "    \n",
    "    \n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10,1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2) \n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"cost\") :\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2>0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha01\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                                       feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate tf1\n",
    "\n",
    "# cd C:\\p\\pythonwork\\ai 실행시키기 편한 위치로 이동 \n",
    "\n",
    "# tensorboard --logdir=./log_dir2/alpha01\n",
    "\n",
    "# DESKTOP-LT49D43 를 localhost로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가설: [[0.72593224]\n",
      " [0.76124233]\n",
      " [0.83109224]\n",
      " [0.8400294 ]\n",
      " [0.6577745 ]\n",
      " [0.72993493]\n",
      " [0.7114668 ]\n",
      " [0.7322839 ]]\n",
      "예측 :[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "정확도 : 0.75\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.01\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = np.array ([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "                    [1, 0, 0], [1, 1, 0], [1, 0 ,1 ], [1, 1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [1], [1], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [8, 3])\n",
    "y = tf.placeholder(tf.float32, shape = [8, 1])\n",
    "\n",
    "# 첫번째 레이어 \n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([3,10]), tf.float32, name=\"weight1\")\n",
    "    b1 = tf.Variable(tf.random_normal([10]),tf.float32, name=\"bias1\")\n",
    "    hypot1 = tf.sigmoid(tf.matmul(X, W1) + b1) # X 실제 입력값\n",
    "    \n",
    "    tf.summary.histogram(\"weight1\", W1)\n",
    "    tf.summary.histogram(\"bias1\", b1)\n",
    "    tf.summary.histogram(\"layer1\", hypot1)\n",
    "    \n",
    "    \n",
    "# 두번째 레이어\n",
    "with tf.name_scope(\"layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([10,1]), tf.float32, name=\"weight2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]),tf.float32, name=\"bias2\")\n",
    "    hypot2 = tf.sigmoid(tf.matmul(hypot1, W2) + b2) \n",
    "    \n",
    "    tf.summary.histogram(\"weight2\", W2)\n",
    "    tf.summary.histogram(\"bias2\", b2)\n",
    "    tf.summary.histogram(\"layer2\", hypot2)\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"cost\") :\n",
    "    cost = -tf.reduce_mean(y * tf.log(hypot2) + (1 - y) * tf.log(1 - hypot2))\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "pred = tf.cast(hypot2>0.5, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(pred, y), dtype=tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"log_dir2/alpha001\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for step in range(5000):\n",
    "        _, h, p, a, summary = sess.run([train, hypot2, pred, accuracy, merged_summary], \n",
    "                                       feed_dict={X:x_data, y:y_data})\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "    print(\"가설: {}\\n예측 :{} \\n정확도 : {}\".format(h, p, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=./log_dir2/alpha001\n",
    "\n",
    "# 같이 비교해서 확인하고 싶을 경우\n",
    "# tensorboard --logdir=./log_dir2  \n",
    "# alpha001과 alpha01을 다 포함하고 있는 폴더까지 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU : Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c73d85a5a6c4>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Master\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Master\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Master\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Master\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Master\\anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777)\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002AEA4EFD608>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002AEA5383988>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002AEA5383748>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 모델 구축 : 성능 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 784]) # 한장의 이미지 : 가로 28 세로 28 28*28=784 (임력데이터)\n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logit = tf.matmul(X, W) + b \n",
    "hypot = tf.nn.softmax(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = y)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypot, 1), tf.argmax(y, 1)) # 실제값 / 예측값\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1        cost: 2.429135024330832\n",
      "epoch: 2        cost: 0.9579243110526695\n",
      "epoch: 3        cost: 0.7731507100842218\n",
      "epoch: 4        cost: 0.6781148779392238\n",
      "epoch: 5        cost: 0.6188663370500913\n",
      "epoch: 6        cost: 0.5756083310192278\n",
      "epoch: 7        cost: 0.5432109648531134\n",
      "epoch: 8        cost: 0.5180204841765489\n",
      "epoch: 9        cost: 0.49698187551715184\n",
      "epoch: 10        cost: 0.47938251137733456\n",
      "epoch: 11        cost: 0.4647093414718452\n",
      "epoch: 12        cost: 0.4513777455416597\n",
      "epoch: 13        cost: 0.4396057274666701\n",
      "epoch: 14        cost: 0.43007063941522067\n",
      "epoch: 15        cost: 0.4198248118162157\n",
      "epoch: 16        cost: 0.4116657164963809\n",
      "epoch: 17        cost: 0.40465867784890247\n",
      "epoch: 18        cost: 0.39702159431847645\n",
      "epoch: 19        cost: 0.390985134840012\n",
      "epoch: 20        cost: 0.3847376897118306\n",
      "epoch: 21        cost: 0.3798220052502374\n",
      "epoch: 22        cost: 0.3744332683086395\n",
      "epoch: 23        cost: 0.3695689398050311\n",
      "epoch: 24        cost: 0.36506772138855687\n",
      "epoch: 25        cost: 0.3605406187339266\n",
      "epoch: 26        cost: 0.3569157721779563\n",
      "epoch: 27        cost: 0.3532399384541946\n",
      "epoch: 28        cost: 0.34946131998842406\n",
      "epoch: 29        cost: 0.3458054562590337\n",
      "epoch: 30        cost: 0.3429731796004556\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c= sess.run([train, cost ], \n",
    "                           feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"epoch:\", (epoch+1), \"       cost:\",avg_cost)\n",
    "        \n",
    "    #print(avg_cost,\"\\t\", a)\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.9106909\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 :\", sess.run(accuracy, feed_dict={X:mnist.train.images, y:mnist.train.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가, 입출력 갯수는 256개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 784]) # 한장의 이미지 : 가로 28 세로 28 28*28=784 (임력데이터)\n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "# 레이어 1개\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1 \n",
    "hypot1 = tf.nn.softmax(logit1)\n",
    "\n",
    "# 레이어 2개\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2 \n",
    "hypot2 = tf.nn.softmax(logit2)\n",
    "\n",
    "# 레이어 3개\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3 \n",
    "hypot3 = tf.nn.softmax(logit3)\n",
    "\n",
    "# 레이어 4개\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit4, labels = y)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1)) # 실제값 / 예측값\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1        cost: 2.351917653517289\n",
      "epoch: 2        cost: 2.3006959160891465\n",
      "epoch: 3        cost: 2.3002749165621665\n",
      "epoch: 4        cost: 2.300161115472967\n",
      "epoch: 5        cost: 2.3000299254330736\n",
      "epoch: 6        cost: 2.299909031607888\n",
      "epoch: 7        cost: 2.2997571607069536\n",
      "epoch: 8        cost: 2.2996012132818064\n",
      "epoch: 9        cost: 2.2994539061459625\n",
      "epoch: 10        cost: 2.299259158047763\n",
      "epoch: 11        cost: 2.299115070863203\n",
      "epoch: 12        cost: 2.2989262381466955\n",
      "epoch: 13        cost: 2.298671007156373\n",
      "epoch: 14        cost: 2.2984552140669394\n",
      "epoch: 15        cost: 2.2981732793287795\n",
      "epoch: 16        cost: 2.2978765626387174\n",
      "epoch: 17        cost: 2.2975084296139823\n",
      "epoch: 18        cost: 2.2971114271337347\n",
      "epoch: 19        cost: 2.296551277854225\n",
      "epoch: 20        cost: 2.2959342549063932\n",
      "epoch: 21        cost: 2.2950540872053677\n",
      "epoch: 22        cost: 2.293870120482011\n",
      "epoch: 23        cost: 2.292364521893587\n",
      "epoch: 24        cost: 2.2898385342684655\n",
      "epoch: 25        cost: 2.286019484780052\n",
      "epoch: 26        cost: 2.279787309820001\n",
      "epoch: 27        cost: 2.2696368182789213\n",
      "epoch: 28        cost: 2.2534034165469077\n",
      "epoch: 29        cost: 2.229945937936957\n",
      "epoch: 30        cost: 2.19921828443354\n",
      "훈련 종료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c= sess.run([train, cost ], \n",
    "                           feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"epoch:\", (epoch+1), \"       cost:\",avg_cost)\n",
    "        \n",
    "    #print(avg_cost,\"\\t\", a)\n",
    "print(\"훈련 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.11234546\n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 :\", sess.run(accuracy, feed_dict={X:mnist.train.images, y:mnist.train.labels}))\n",
    "# Vanishing gradient 현상을 피해갈 수 없다.\n",
    "# 레이어를 더 깊게 추가 해도 달라지지는 않고 Vanishing gradient 현상이 더 심해질 수도 있다.\n",
    "# 이를 보완하기 위해 나온것이 ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어 3개 추가, 입출력 갯수는 256개 : Relu 사용 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1        cost: 3.0162841857563363\n",
      "epoch: 2        cost: 1.4736104466698394\n",
      "epoch: 3        cost: 1.1444314811446454\n",
      "epoch: 4        cost: 1.0267661666870112\n",
      "epoch: 5        cost: 0.8982908309589732\n",
      "epoch: 6        cost: 0.7887494499033149\n",
      "epoch: 7        cost: 0.7484260391105304\n",
      "epoch: 8        cost: 0.7160093752904368\n",
      "epoch: 9        cost: 0.6640681561556735\n",
      "epoch: 10        cost: 0.6344313610683794\n",
      "epoch: 11        cost: 0.6076449167728423\n",
      "epoch: 12        cost: 0.5724321880123828\n",
      "epoch: 13        cost: 0.570062628876079\n",
      "epoch: 14        cost: 0.5466190311041745\n",
      "epoch: 15        cost: 0.5660762833465229\n",
      "epoch: 16        cost: 0.5748557220805774\n",
      "epoch: 17        cost: 0.5719650824503468\n",
      "epoch: 18        cost: 0.5159773915464227\n",
      "epoch: 19        cost: 0.5106882611188022\n",
      "epoch: 20        cost: 0.49534495917233556\n",
      "epoch: 21        cost: 0.4789396319606084\n",
      "epoch: 22        cost: 0.46753696463324784\n",
      "epoch: 23        cost: 0.45980866616422456\n",
      "epoch: 24        cost: 0.4350188658454203\n",
      "epoch: 25        cost: 0.431131445657123\n",
      "epoch: 26        cost: 0.42084317061034093\n",
      "epoch: 27        cost: 0.4295560214194387\n",
      "epoch: 28        cost: 0.41703063910657706\n",
      "epoch: 29        cost: 0.41578598119995785\n",
      "epoch: 30        cost: 0.4142579275911505\n",
      "훈련 종료\n",
      "정확도 : 0.8644\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 784]) \n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "# 레이어 1개\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1 \n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "# 레이어 2개\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2 \n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "# 레이어 3개\n",
    "W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3 \n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "# 레이어 4개\n",
    "W4 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit4, labels = y)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c= sess.run([train, cost ], \n",
    "                           feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"epoch:\", (epoch+1), \"       cost:\",avg_cost)\n",
    "        \n",
    "    #print(avg_cost,\"\\t\", a)\n",
    "print(\"훈련 종료\")\n",
    "print(\"정확도 :\", sess.run(accuracy, feed_dict={X:mnist.train.images, y:mnist.train.labels}))\n",
    "\n",
    "# 전부다 relu로 하면 발산이 되버리기때문에 적절하게 사용하여야 한다.\n",
    "# relu를 통해 처음 어처구니 없던 정확도를 어느정도 보완할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavier 초기화 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1        cost: 1.0683598891171537\n",
      "epoch: 2        cost: 0.38512185058810494\n",
      "epoch: 3        cost: 0.30627703720873023\n",
      "epoch: 4        cost: 0.2706237544254824\n",
      "epoch: 5        cost: 0.24258545577526083\n",
      "epoch: 6        cost: 0.21078521571376108\n",
      "epoch: 7        cost: 0.19949925159866141\n",
      "epoch: 8        cost: 0.1736085562814366\n",
      "epoch: 9        cost: 0.1583704393831166\n",
      "epoch: 10        cost: 0.14809251966801562\n",
      "epoch: 11        cost: 0.13309905114499013\n",
      "epoch: 12        cost: 0.12208236125382511\n",
      "epoch: 13        cost: 0.11493035052310331\n",
      "epoch: 14        cost: 0.10902121044018051\n",
      "epoch: 15        cost: 0.09949159603227269\n",
      "epoch: 16        cost: 0.09409110658548087\n",
      "epoch: 17        cost: 0.08733416297219011\n",
      "epoch: 18        cost: 0.08524971466850154\n",
      "epoch: 19        cost: 0.07585078477859501\n",
      "epoch: 20        cost: 0.07349518322809177\n",
      "epoch: 21        cost: 0.06820624801245603\n",
      "epoch: 22        cost: 0.06557697756723924\n",
      "epoch: 23        cost: 0.06183602514592083\n",
      "epoch: 24        cost: 0.06046686419031841\n",
      "epoch: 25        cost: 0.05267407674342395\n",
      "epoch: 26        cost: 0.05201458079571074\n",
      "epoch: 27        cost: 0.047930833355269674\n",
      "epoch: 28        cost: 0.04630145205354146\n",
      "epoch: 29        cost: 0.043994331051680186\n",
      "epoch: 30        cost: 0.03975061519918116\n",
      "훈련 종료\n",
      "정확도 : 0.98301816\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 784]) \n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "# 레이어 1개\n",
    "W1 = tf.get_variable(\"W11\", shape = [784, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1 \n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "# 레이어 2개\n",
    "W2 = tf.get_variable(\"W12\", shape= [256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2 \n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "# 레이어 3개\n",
    "W3 = tf.get_variable(\"W13\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3 \n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "# 레이어 4개\n",
    "W4 = tf.get_variable(\"W14\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit4, labels = y)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c= sess.run([train, cost ], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"epoch:\", (epoch+1), \"       cost:\",avg_cost)\n",
    "        \n",
    "    #print(avg_cost,\"\\t\", a)\n",
    "print(\"훈련 종료\")\n",
    "print(\"정확도 :\", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "# 초기화를 단순히 랜덤하게 맡기지 않고 xavier를 사용해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.031508215340701\n",
      "epoch: 2     cost: 0.3790736255862495\n",
      "epoch: 3     cost: 0.30814987870779886\n",
      "epoch: 4     cost: 0.26801272115924135\n",
      "epoch: 5     cost: 0.2376656941663135\n",
      "epoch: 6     cost: 0.21096478543498284\n",
      "epoch: 7     cost: 0.19156267342242314\n",
      "epoch: 8     cost: 0.17260836240920155\n",
      "epoch: 9     cost: 0.1580445815216412\n",
      "epoch: 10     cost: 0.14556202324953943\n",
      "epoch: 11     cost: 0.13353020947087882\n",
      "epoch: 12     cost: 0.12291292048313403\n",
      "epoch: 13     cost: 0.11465030687776483\n",
      "epoch: 14     cost: 0.10611131792718716\n",
      "epoch: 15     cost: 0.10017247533256352\n",
      "epoch: 16     cost: 0.09273072286085644\n",
      "epoch: 17     cost: 0.08726274440234354\n",
      "epoch: 18     cost: 0.08195068756965075\n",
      "epoch: 19     cost: 0.07672821264375339\n",
      "epoch: 20     cost: 0.07286044915291395\n",
      "epoch: 21     cost: 0.06909173448993401\n",
      "epoch: 22     cost: 0.06432922134345229\n",
      "epoch: 23     cost: 0.06059519200839782\n",
      "epoch: 24     cost: 0.057387893809513625\n",
      "epoch: 25     cost: 0.0537908845191652\n",
      "epoch: 26     cost: 0.05215525597672567\n",
      "epoch: 27     cost: 0.04880498950454321\n",
      "epoch: 28     cost: 0.045743376067416235\n",
      "epoch: 29     cost: 0.04317292955788702\n",
      "epoch: 30     cost: 0.03963202609257264\n",
      "훈련 종료\n",
      "정확도 :  0.9748\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 256], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.sigmoid(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[256, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.softmax(logit4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot4, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좀 더 deep 하고 wide하게 : layer는 총8개로 구성 입출력 갯수는 512개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 1.9995563301173125\n",
      "epoch: 2     cost: 0.537019267244772\n",
      "epoch: 3     cost: 0.29130739499222147\n",
      "epoch: 4     cost: 0.2261093508384444\n",
      "epoch: 5     cost: 0.18427790677005582\n",
      "epoch: 6     cost: 0.1482327699661255\n",
      "epoch: 7     cost: 0.12498529689555808\n",
      "epoch: 8     cost: 0.11033148137005902\n",
      "epoch: 9     cost: 0.0965474120121111\n",
      "epoch: 10     cost: 0.0855373382229697\n",
      "epoch: 11     cost: 0.07365556404671886\n",
      "epoch: 12     cost: 0.06758813954551114\n",
      "epoch: 13     cost: 0.059849932457913056\n",
      "epoch: 14     cost: 0.05014159352603283\n",
      "epoch: 15     cost: 0.6189677602628411\n",
      "epoch: 16     cost: 0.09447358403016218\n",
      "epoch: 17     cost: 0.0640528749098832\n",
      "epoch: 18     cost: 0.047479332565245294\n",
      "epoch: 19     cost: 0.03953024336052212\n",
      "epoch: 20     cost: 0.07442903707650572\n",
      "epoch: 21     cost: 0.026675700215961454\n",
      "epoch: 22     cost: 0.02285046041096476\n",
      "epoch: 23     cost: 0.019833356564525857\n",
      "epoch: 24     cost: 0.41946482175537814\n",
      "epoch: 25     cost: 0.0648250435360453\n",
      "epoch: 26     cost: 0.03532199348407713\n",
      "epoch: 27     cost: 0.022445132647725654\n",
      "epoch: 28     cost: 0.017823074476962734\n",
      "epoch: 29     cost: 0.013334597520403214\n",
      "epoch: 30     cost: 0.010954754691655674\n",
      "훈련 종료\n",
      "정확도 :  0.9781\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1        cost: 1.936471946456215\n",
      "epoch: 2        cost: 0.5285280999270349\n",
      "epoch: 3        cost: 0.3032230417565866\n",
      "epoch: 4        cost: 0.21610232033512813\n",
      "epoch: 5        cost: 0.17493998045271078\n",
      "epoch: 6        cost: 0.1435517531362447\n",
      "epoch: 7        cost: 0.12268285789273004\n",
      "epoch: 8        cost: 0.1186514434421604\n",
      "epoch: 9        cost: 0.09707866492596531\n",
      "epoch: 10        cost: 0.08570618947121232\n",
      "epoch: 11        cost: 0.0712030697275292\n",
      "epoch: 12        cost: 0.06380111257460983\n",
      "epoch: 13        cost: 0.06149713485417042\n",
      "epoch: 14        cost: 0.04915748389437796\n",
      "epoch: 15        cost: 0.04380656268616968\n",
      "epoch: 16        cost: 0.46174525495957214\n",
      "epoch: 17        cost: 0.07434221578592602\n",
      "epoch: 18        cost: 0.0540956815704703\n",
      "epoch: 19        cost: 0.10194004181264471\n",
      "epoch: 20        cost: 0.03741879404607143\n",
      "epoch: 21        cost: 0.029292250727726656\n",
      "epoch: 22        cost: 0.024227161432023737\n",
      "epoch: 23        cost: 0.6180427018883213\n",
      "epoch: 24        cost: 0.07219584289938213\n",
      "epoch: 25        cost: 0.039438283350318674\n",
      "epoch: 26        cost: 0.028887822112407188\n",
      "epoch: 27        cost: 0.02049581224014137\n",
      "epoch: 28        cost: 0.016212548701177273\n",
      "epoch: 29        cost: 0.011801493604963815\n",
      "epoch: 30        cost: 0.009831584110771391\n",
      "훈련 종료\n",
      "정확도 : 0.9777\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape = [None, 784]) \n",
    "y = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "# 레이어 1개\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1 \n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "\n",
    "# 레이어 2개\n",
    "W2 = tf.get_variable(\"W2\", shape= [512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2 \n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "\n",
    "# 레이어 3개\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3 \n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "\n",
    "# 레이어 4개\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "\n",
    "# 레이어 4개\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "\n",
    "# 레이어 4개\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "\n",
    "# 레이어 4개\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "\n",
    "# 레이어 4개\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit8, labels = y)) \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c= sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys})\n",
    "        avg_cost += c/total_batch \n",
    "    print(\"epoch:\", (epoch+1), \"       cost:\",avg_cost)\n",
    "        \n",
    "    #print(avg_cost,\"\\t\", a)\n",
    "print(\"훈련 종료\")\n",
    "print(\"정확도 :\", sess.run(accuracy, feed_dict={X:mnist.test.images, \n",
    "                                             y:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout : 과적합 해결 방안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 2.351242759877985\n",
      "epoch: 2     cost: 1.6446034570173773\n",
      "epoch: 3     cost: 0.9068215890364221\n",
      "epoch: 4     cost: 0.5576108811118387\n",
      "epoch: 5     cost: 0.4182804959470575\n",
      "epoch: 6     cost: 0.34543202709067944\n",
      "epoch: 7     cost: 0.2973410631851718\n",
      "epoch: 8     cost: 0.25990512184121384\n",
      "epoch: 9     cost: 0.23423099284822263\n",
      "epoch: 10     cost: 0.21219831008802756\n",
      "epoch: 11     cost: 0.198169766054912\n",
      "epoch: 12     cost: 0.18157902695915928\n",
      "epoch: 13     cost: 0.16950327678160243\n",
      "epoch: 14     cost: 0.16319137643684045\n",
      "epoch: 15     cost: 0.1517579479786482\n",
      "epoch: 16     cost: 0.14421671748161313\n",
      "epoch: 17     cost: 0.13907569799233568\n",
      "epoch: 18     cost: 0.12694424377246333\n",
      "epoch: 19     cost: 0.12891812384128562\n",
      "epoch: 20     cost: 0.11746050463481388\n",
      "epoch: 21     cost: 0.11153156101703646\n",
      "epoch: 22     cost: 0.10841762808236212\n",
      "epoch: 23     cost: 0.10702144416218451\n",
      "epoch: 24     cost: 0.10155249421569433\n",
      "epoch: 25     cost: 0.09995576831427493\n",
      "epoch: 26     cost: 0.09061482610350306\n",
      "epoch: 27     cost: 0.09027569885281002\n",
      "epoch: 28     cost: 0.08901848084547298\n",
      "epoch: 29     cost: 0.08634234092452303\n",
      "epoch: 30     cost: 0.08283630253916438\n",
      "훈련 종료\n",
      "정확도 :  0.9768\n",
      "정확도 :  0.9903273\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels, prob:1}))\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.train.images,\n",
    "                                             y:mnist.train.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout : 과적합 해결 방안 + AdamOptimizer : 98.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1     cost: 0.915840327035297\n",
      "epoch: 2     cost: 0.24557315539230012\n",
      "epoch: 3     cost: 0.17753526167436082\n",
      "epoch: 4     cost: 0.14141242029992007\n",
      "epoch: 5     cost: 0.12486829819327044\n",
      "epoch: 6     cost: 0.11424899634989827\n",
      "epoch: 7     cost: 0.09944054115902295\n",
      "epoch: 8     cost: 0.08908399126069108\n",
      "epoch: 9     cost: 0.08470075954767793\n",
      "epoch: 10     cost: 0.08021711705760518\n",
      "epoch: 11     cost: 0.07195800206200638\n",
      "epoch: 12     cost: 0.06990415666591034\n",
      "epoch: 13     cost: 0.0660725773837079\n",
      "epoch: 14     cost: 0.06563888585025615\n",
      "epoch: 15     cost: 0.06464548937976365\n",
      "epoch: 16     cost: 0.05487026819451293\n",
      "epoch: 17     cost: 0.055381746464832286\n",
      "epoch: 18     cost: 0.05593223665756254\n",
      "epoch: 19     cost: 0.055760500138117494\n",
      "epoch: 20     cost: 0.05074481328864666\n",
      "epoch: 21     cost: 0.04767570676536044\n",
      "epoch: 22     cost: 0.04959113811904731\n",
      "epoch: 23     cost: 0.04963607249950821\n",
      "epoch: 24     cost: 0.045947305536405636\n",
      "epoch: 25     cost: 0.048637508567083904\n",
      "epoch: 26     cost: 0.04446964392269201\n",
      "epoch: 27     cost: 0.0409556049387902\n",
      "epoch: 28     cost: 0.04523076103661544\n",
      "epoch: 29     cost: 0.03978931448761035\n",
      "epoch: 30     cost: 0.04073746727440845\n",
      "훈련 종료\n",
      "정확도 :  0.9831\n",
      "정확도 :  0.99825454\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "logit1 = tf.matmul(X, W1) + b1\n",
    "hypot1 = tf.nn.relu(logit1)\n",
    "hypot1 = tf.nn.dropout(hypot1, keep_prob=prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "logit2 = tf.matmul(hypot1, W2) + b2\n",
    "hypot2 = tf.nn.relu(logit2)\n",
    "hypot2 = tf.nn.dropout(hypot2, keep_prob=prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "logit3 = tf.matmul(hypot2, W3) + b3\n",
    "hypot3 = tf.nn.relu(logit3)\n",
    "hypot3 = tf.nn.dropout(hypot3, keep_prob=prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "logit4 = tf.matmul(hypot3, W4) + b4\n",
    "hypot4 = tf.nn.relu(logit4)\n",
    "hypot4 = tf.nn.dropout(hypot4, keep_prob=prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([512]))\n",
    "logit5 = tf.matmul(hypot4, W5) + b5\n",
    "hypot5 = tf.nn.relu(logit5)\n",
    "hypot5 = tf.nn.dropout(hypot5, keep_prob=prob)\n",
    "\n",
    "W6 = tf.get_variable(\"W6\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b6 = tf.Variable(tf.random_normal([512]))\n",
    "logit6 = tf.matmul(hypot5, W6) + b6\n",
    "hypot6 = tf.nn.relu(logit6)\n",
    "hypot6 = tf.nn.dropout(hypot6, keep_prob=prob)\n",
    "\n",
    "W7 = tf.get_variable(\"W7\", shape=[512, 512], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b7 = tf.Variable(tf.random_normal([512]))\n",
    "logit7 = tf.matmul(hypot6, W7) + b7\n",
    "hypot7 = tf.nn.relu(logit7)\n",
    "hypot7 = tf.nn.dropout(hypot7, keep_prob=prob)\n",
    "\n",
    "W8 = tf.get_variable(\"W8\", shape=[512, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b8 = tf.Variable(tf.random_normal([10]))\n",
    "logit8 = tf.matmul(hypot7, W8) + b8\n",
    "hypot8 = tf.nn.softmax(logit8)\n",
    "hypot8 = tf.nn.dropout(hypot8, keep_prob=prob)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit8, labels=y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypot8,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "training_epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([train, cost], feed_dict={X:batch_xs, y:batch_ys, prob:0.7})\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"epoch:\", (epoch+1), \"    cost:\", avg_cost)\n",
    "\n",
    "print(\"훈련 종료\")\n",
    "\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.test.images,\n",
    "                                             y:mnist.test.labels, prob:1}))\n",
    "print(\"정확도 : \", sess.run(accuracy, feed_dict={X:mnist.train.images,\n",
    "                                             y:mnist.train.labels, prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
